{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5583db-a63b-4fd6-9705-65eb78f4d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from model import EmoTalk\n",
    "from types import SimpleNamespace\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31ccfb-3c23-43f2-ae1d-509356272cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmoTalk 数据集类\n",
    "class EmoTalkDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_files, blendshape_files, device):\n",
    "        # 初始化音频文件和blendshape文件\n",
    "        self.audio_files = audio_files\n",
    "        self.blendshape_files = blendshape_files\n",
    "        self.device = device\n",
    "        self.file_pairs = list(zip(audio_files,\n",
    "                                   blendshape_files))  # 将音频和blendshape文件配对\n",
    "        random.shuffle(self.file_pairs)  # 初始时打乱文件对\n",
    "        self.used_pairs = set()  # 用于追踪在一个轮次中已使用的文件对\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回样本的数量，每个样本由两个音频文件组成\n",
    "        return len(self.file_pairs) // 2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 当轮次结束时，清空已使用的文件对\n",
    "        if len(self.used_pairs) >= len(self.file_pairs) - 1:\n",
    "            self.used_pairs.clear()\n",
    "\n",
    "        while True:\n",
    "            # 随机选择两个不同的文件对\n",
    "            pair_indices = random.sample(range(len(self.file_pairs)), 2)\n",
    "            pair_1, pair_2 = [self.file_pairs[i] for i in pair_indices]\n",
    "\n",
    "            # 确保选择的文件对是唯一的，并且在本轮中未被使用\n",
    "            if (pair_1, pair_2) not in self.used_pairs and (\n",
    "                    pair_2, pair_1) not in self.used_pairs:\n",
    "                self.used_pairs.add((pair_1, pair_2))  # 添加到已使用的文件对集合\n",
    "                break\n",
    "\n",
    "        # 加载第一个音频文件和对应的blendshape\n",
    "        wav_path_1, blendshape_path_1 = pair_1\n",
    "        speech_array_1, sampling_rate = librosa.load(wav_path_1, sr=16000)\n",
    "        audio_1 = torch.FloatTensor(speech_array_1).unsqueeze(0).to(\n",
    "            self.device)\n",
    "\n",
    "        blendshape_1 = np.load(blendshape_path_1)\n",
    "        blendshape_tensor_1 = torch.tensor(blendshape_1,\n",
    "                                           device=self.device,\n",
    "                                           dtype=torch.float32)\n",
    "\n",
    "        # 加载第二个音频文件和对应的blendshape\n",
    "        wav_path_2, blendshape_path_2 = pair_2\n",
    "        speech_array_2, sampling_rate = librosa.load(wav_path_2, sr=16000)\n",
    "        audio_2 = torch.FloatTensor(speech_array_2).unsqueeze(0).to(\n",
    "            self.device)\n",
    "\n",
    "        blendshape_2 = np.load(blendshape_path_2)\n",
    "        blendshape_tensor_2 = torch.tensor(blendshape_2,\n",
    "                                           device=self.device,\n",
    "                                           dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"input12\": audio_1,  # 输入12\n",
    "            \"input21\": audio_2,  # 输入21\n",
    "            \"target11\": blendshape_tensor_1,  # 目标11\n",
    "            \"target12\": blendshape_tensor_2,  # 目标12\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52d775-e1d3-4f86-8767-7234429b42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EmoTalk 损失类\n",
    "class EmotalkLoss():\n",
    "\n",
    "    def __init__(self, args):\n",
    "        # 初始化损失权重参数\n",
    "        self.lambda_cross = args.lambda_cross\n",
    "        self.lambda_self = args.lambda_self\n",
    "        self.lambda_velocity = args.lambda_velocity\n",
    "        self.lambda_cls = args.lambda_cls\n",
    "        self.mse_loss = nn.MSELoss()  # 均方误差损失\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()  # 交叉熵损失\n",
    "\n",
    "    def Loss(self, outputs, targets, labels):\n",
    "        # 提取输出和目标张量\n",
    "        D12 = outputs[\"output12\"]\n",
    "        D21 = outputs[\"output21\"]\n",
    "        D11 = outputs[\"output11\"]\n",
    "        B1 = targets[\"target1\"]\n",
    "        B2 = targets[\"target2\"]\n",
    "\n",
    "        # 交叉重构损失\n",
    "        L_cross = self.mse_loss(D12, B1) + self.mse_loss(D21, B2)\n",
    "\n",
    "        # 自我重构损失\n",
    "        L_self = self.mse_loss(D11, B1)\n",
    "\n",
    "        # 速度损失\n",
    "        velocity_gt = B1[:, 1:] - B1[:, :-1]  # 真实速度\n",
    "        velocity_pred = D12[:, 1:] - D12[:, :-1]  # 预测速度\n",
    "        L_velocity = self.mse_loss(velocity_gt, velocity_pred)\n",
    "\n",
    "        # 分类损失\n",
    "        # L_cls = self.cross_entropy_loss(D12, labels)\n",
    "\n",
    "        # 合计总损失\n",
    "        loss = (L_cross * self.lambda_cross + L_self * self.lambda_self +\n",
    "                L_velocity * self.lambda_velocity)  # + L_cls * self.lambda_cls\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a30a1-3ddd-4621-97ff-4096e95f299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播的函数\n",
    "def forward_pass(model, inputs12, inputs21, targets11, targets12, device):\n",
    "    # 执行模型的前向传播\n",
    "\n",
    "    #用于交叉重构损失和速度损失\n",
    "    bs_output11, bs_output12, label1 = model({\n",
    "        \"input12\":\n",
    "        inputs12,\n",
    "        \"input21\":\n",
    "        inputs21,\n",
    "        \"target11\":\n",
    "        targets11,\n",
    "        \"target12\":\n",
    "        targets12,\n",
    "        \"level\":\n",
    "        torch.tensor([1]).to(device),\n",
    "        \"person\":\n",
    "        torch.tensor([0], device=device),\n",
    "    })\n",
    "\n",
    "    #用于交叉重构损失\n",
    "    bs_output21, bs_output22, label2 = model({\n",
    "        \"input12\":\n",
    "        inputs21,\n",
    "        \"input21\":\n",
    "        inputs12,\n",
    "        \"target11\":\n",
    "        targets12,\n",
    "        \"target12\":\n",
    "        targets11,\n",
    "        \"level\":\n",
    "        torch.tensor([1]).to(device),\n",
    "        \"person\":\n",
    "        torch.tensor([0], device=device),\n",
    "    })\n",
    "\n",
    "    #用于速度损失\n",
    "    bs_output_self1, _, label3 = model({\n",
    "        \"input12\":\n",
    "        inputs12,\n",
    "        \"input21\":\n",
    "        inputs12,\n",
    "        \"target11\":\n",
    "        targets11,\n",
    "        \"target12\":\n",
    "        targets11,\n",
    "        \"level\":\n",
    "        torch.tensor([1]).to(device),\n",
    "        \"person\":\n",
    "        torch.tensor([0], device=device),\n",
    "    })\n",
    "\n",
    "    # 返回输出和标签\n",
    "    return {\n",
    "        \"output12\": bs_output11,\n",
    "        \"output21\": bs_output21,\n",
    "        \"output11\": bs_output_self1\n",
    "    }, [label1, label2, label3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927a1d0-6ea8-4dc0-85b3-b3298c5c79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载checkpoint的函数\n",
    "def load_checkpoint(model_path, model, optimizer, device):\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # 定义checkpoint文件的目录和基础名称\n",
    "    model_dir = Path(model_path).parent\n",
    "    model_name = Path(model_path).stem  # 获取文件名不带扩展名\n",
    "\n",
    "    # 构建匹配checkpoint文件的模式\n",
    "    checkpoint_pattern = f\"{model_name}_epoch*.pth\"\n",
    "\n",
    "    # 尝试找到最新的checkpoint文件\n",
    "    checkpoint_files = list(model_dir.glob(checkpoint_pattern))\n",
    "\n",
    "    if checkpoint_files:\n",
    "        # 直接加载检测到的checkpoint文件\n",
    "        checkpoint_file = checkpoint_files[0]\n",
    "\n",
    "        print(f\"Loading model parameters from {checkpoint_file}\")\n",
    "        checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])  # 加载模型状态\n",
    "        optimizer.load_state_dict(\n",
    "            checkpoint['optimizer_state_dict'])  # 加载优化器状态\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['loss']\n",
    "\n",
    "        print(\n",
    "            f\"Resuming training from epoch {start_epoch} with last loss: {best_loss:.4f}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"No existing model found. Initializing new model.\")\n",
    "\n",
    "    return start_epoch, best_loss, model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3038e-26c3-4aed-9142-0b9ebe361f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存checkpoint的函数\n",
    "def save_checkpoint(epoch, model, optimizer, loss, model_path):\n",
    "    # 生成时间戳并将epoch数包含在checkpoint中\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    new_model_path = f\"{model_path}_epoch{epoch}_{timestamp}.pth\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(new_model_path), exist_ok=True)  # 确保保存目录存在\n",
    "\n",
    "    try:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }\n",
    "        torch.save(checkpoint, new_model_path)  # 保存checkpoint\n",
    "        print(f'Checkpoint saved to {new_model_path} at {datetime.now()}')\n",
    "\n",
    "        # 如果存在前一轮的checkpoint，则删除它\n",
    "        if epoch > 1:  # 确保在第二轮之前不会尝试删除\n",
    "            prev_epoch = epoch - 1\n",
    "            prev_model_pattern = f\"{model_path}_epoch{prev_epoch}_*.pth\"\n",
    "            prev_model_files = list(Path('.').glob(prev_model_pattern))\n",
    "\n",
    "            for prev_model_file in prev_model_files:\n",
    "                os.remove(prev_model_file)  # 删除旧的checkpoint\n",
    "                print(f\"Removed old checkpoint at {prev_model_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361eee25-abdb-499e-8084-e148cfe09747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型的函数\n",
    "def train_model(args):\n",
    "    model = EmoTalk(args).to(args.device)  # 初始化模型并将其放置到设备上\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)  # 使用 Adam 优化器\n",
    "\n",
    "    # 如果存在checkpoint，则加载\n",
    "    start_epoch, best_loss, model, optimizer = load_checkpoint(\n",
    "        args.model_path, model, optimizer, args.device)\n",
    "\n",
    "    dataset = EmoTalkDataset(audio_files=args.audio_files,\n",
    "                             blendshape_files=args.blendshape_files,\n",
    "                             device=args.device)  # 初始化数据集\n",
    "    dataloader = DataLoader(dataset, batch_size=1,\n",
    "                            shuffle=False)  # 使用 DataLoader 进行批处理\n",
    "    num_epochs = args.epochs\n",
    "    Loss = EmotalkLoss(args)  # 初始化损失对象\n",
    "    model.train()  # 设置模型为训练模式\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        running_loss = 0.0\n",
    "        total_batches = 0\n",
    "\n",
    "        # 在每个轮次开始时重置已使用的文件对\n",
    "        dataset.used_pairs.clear()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            # 获取输入、目标值\n",
    "            inputs12 = batch[\"input12\"]\n",
    "            inputs21 = batch[\"input21\"]\n",
    "            targets11 = batch[\"target11\"]\n",
    "            targets12 = batch[\"target12\"]\n",
    "\n",
    "            # 清零梯度\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 前向传播\n",
    "            outputs, labels = forward_pass(model, inputs12, inputs21,\n",
    "                                           targets11, targets12, args.device)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = Loss.Loss(outputs, {\n",
    "                \"target1\": targets11,\n",
    "                \"target2\": targets12\n",
    "            }, labels[0])\n",
    "\n",
    "            # 反向传播和参数更新\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计损失\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / total_batches if total_batches > 0 else float(\n",
    "            'inf')  # 计算平均损失\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 每个轮次结束后保存checkpoint\n",
    "        save_checkpoint(epoch + 1, model, optimizer, avg_loss, args.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f788c6-de4e-4d86-a6bd-e8f3a55172c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主程序\n",
    "def main():\n",
    "\n",
    "    audio_files = list(Path('audio').glob('*.wav'))  # 获取所有音频文件\n",
    "    blendshape_files = list(Path('result').glob('*.npy'))  # 获取所有blendshape文件\n",
    "\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\")  # 选择设备\n",
    "\n",
    "    # 设置训练参数\n",
    "    args = SimpleNamespace(\n",
    "        audio_files=audio_files,\n",
    "        blendshape_files=blendshape_files,\n",
    "        feature_dim=832,\n",
    "        bs_dim=52,\n",
    "        max_seq_len=5000,\n",
    "        period=30,\n",
    "        batch_size=1,  # 设为1以支持可变长度序列\n",
    "        device=device,\n",
    "        lr=1e-4,  # 学习率\n",
    "        lambda_cross=1.0,  # 交叉重构损失的权重\n",
    "        lambda_self=1.0,  # 自我重构损失的权重\n",
    "        lambda_velocity=0.5,  # 速度损失的权重\n",
    "        lambda_cls=1.0,  # 分类损失的权重\n",
    "        epochs=80,  # 训练的总轮次\n",
    "        model_path='pretrain_model/emotalk_model')  # 模型保存路径\n",
    "\n",
    "    train_model(args)  # 开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d2184-6cbf-461b-b437-29a39e864f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/PyTorch-2.0.0/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at models/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.lm_head.bias', 'wav2vec2.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at models/wav2vec-english-speech-emotion-recognition and are newly initialized: ['wav2vec2.lm_head.bias', 'wav2vec2.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters from pretrain_model/emotalk_model_epoch76_20241212_004832.pth\n",
      "Resuming training from epoch 76 with last loss: 0.0032\n",
      "Epoch [77/80], Loss: 0.0031\n",
      "Checkpoint saved to pretrain_model/emotalk_model_epoch77_20241212_094131.pth at 2024-12-12 09:41:41.336606\n",
      "Removed old checkpoint at pretrain_model/emotalk_model_epoch76_20241212_004832.pth\n",
      "Epoch [78/80], Loss: 0.0030\n",
      "Checkpoint saved to pretrain_model/emotalk_model_epoch78_20241212_094959.pth at 2024-12-12 09:50:08.933290\n",
      "Removed old checkpoint at pretrain_model/emotalk_model_epoch77_20241212_094131.pth\n",
      "Epoch [79/80], Loss: 0.0030\n",
      "Checkpoint saved to pretrain_model/emotalk_model_epoch79_20241212_095827.pth at 2024-12-12 09:58:36.889076\n",
      "Removed old checkpoint at pretrain_model/emotalk_model_epoch78_20241212_094959.pth\n",
      "Epoch [80/80], Loss: 0.0026\n",
      "Checkpoint saved to pretrain_model/emotalk_model_epoch80_20241212_100708.pth at 2024-12-12 10:07:17.678854\n",
      "Removed old checkpoint at pretrain_model/emotalk_model_epoch79_20241212_095827.pth\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45a3a5-aa0d-491c-ac5c-fab6e3c358a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
